{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply preprocessing on utterance \n",
    "apply one hot encoding on intent(which is converted into sentence)\n",
    "\n",
    "do the prediction and display the intent and not the one hot encoded message \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "TF-IDF Logistic Regression Model Accuracy: 0.9842\n",
      "TF-IDF Decision Tree Model Accuracy: 0.9765\n",
      "TF-IDF Naive Bayes Model Accuracy: 0.9587\n",
      "TF-IDF SVC Model Accuracy: 0.9835\n",
      "TF-IDF Random Forest Model Accuracy: 0.9872\n",
      "TF-IDF XGBoost Model Accuracy: 0.9796\n",
      "TF-IDF AdaBoost Model Accuracy: 0.2584\n",
      "WARNING:tensorflow:From c:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "485/485 [==============================] - 3s 4ms/step - loss: 1.4038 - accuracy: 0.6356 - val_loss: 0.3054 - val_accuracy: 0.9402\n",
      "Epoch 2/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.3480 - accuracy: 0.9105 - val_loss: 0.1096 - val_accuracy: 0.9756\n",
      "Epoch 3/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.1920 - accuracy: 0.9487 - val_loss: 0.0605 - val_accuracy: 0.9826\n",
      "Epoch 4/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.1337 - accuracy: 0.9616 - val_loss: 0.0405 - val_accuracy: 0.9878\n",
      "Epoch 5/10\n",
      "485/485 [==============================] - 2s 3ms/step - loss: 0.0980 - accuracy: 0.9727 - val_loss: 0.0338 - val_accuracy: 0.9843\n",
      "Epoch 6/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.0775 - accuracy: 0.9774 - val_loss: 0.0305 - val_accuracy: 0.9872\n",
      "Epoch 7/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.0618 - accuracy: 0.9816 - val_loss: 0.0293 - val_accuracy: 0.9901\n",
      "Epoch 8/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.0553 - accuracy: 0.9815 - val_loss: 0.0259 - val_accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.0481 - accuracy: 0.9856 - val_loss: 0.0285 - val_accuracy: 0.9896\n",
      "Epoch 10/10\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 0.0450 - accuracy: 0.9859 - val_loss: 0.0242 - val_accuracy: 0.9919\n",
      "TF-IDF Deep Learning Dense Neural Network Model Accuracy: 0.9879\n",
      "Epoch 1/10\n",
      " 22/485 [>.............................] - ETA: 5:08:37 - loss: 3.0030 - accuracy: 0.1804"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 150\u001b[0m\n\u001b[0;32m    144\u001b[0m dl_models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense Neural Network\u001b[39m\u001b[38;5;124m'\u001b[39m: build_dl_model(X_train_tfidf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m: build_dl_lstm_model(X_train_tfidf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    147\u001b[0m }\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m dl_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     _, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_tfidf, y_test_encoded, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF-IDF Deep Learning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Model Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "# Assuming necessary NLTK data files are already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = '20000-Utterances-Training-dataset-for-chatbots-virtual-assistant-Bitext-sample.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['category', 'flags']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Define the mapping dictionary\n",
    "intent_mapping = {\n",
    "    'create_account': \"To create a new account, go to the sign-up page and fill in your details.\",\n",
    "    'delete_account': \"To delete your account, navigate to your account settings and select 'Delete Account'.\",\n",
    "    'edit_account': \"To update your account details, go to your profile and select 'Edit Profile'.\",\n",
    "    'recover_password': \"To recover your password, click on 'Forgot Password' and follow the instructions.\",\n",
    "    'registration_problems': \"If you have registration issues, check the provided information and ensure all required fields are filled.\",\n",
    "    'switch_account': \"To switch accounts, log out of your current account and log in with the other account credentials.\",\n",
    "    'check_cancellation_fee': \"To check the cancellation fee, visit the cancellation policy section in our terms and conditions.\",\n",
    "    'contact_customer_service': \"To contact customer service, use the 'Contact Us' form on our website.\",\n",
    "    'contact_human_agent': \"To chat with a human agent, use the live chat feature available on our support page.\",\n",
    "    'delivery_options': \"To view delivery options, go to the shipping section during checkout.\",\n",
    "    'delivery_period': \"To check the delivery period, view the estimated delivery date provided at checkout.\",\n",
    "    'complaint': \"To file a complaint, fill out the complaint form available on our support page.\",\n",
    "    'review': \"To leave a review, go to the product page and click on 'Write a Review'.\",\n",
    "    'check_invoices': \"To check your invoices, log in to your account and go to the 'Invoices' section.\",\n",
    "    'get_invoice': \"To get a copy of your invoice, access the 'Orders' section in your account and select 'View Invoice'.\",\n",
    "    'newsletter_subscription': \"To subscribe to our newsletter, enter your email in the subscription box at the bottom of the homepage.\",\n",
    "    'cancel_order': \"To cancel your order, go to your order history and select 'Cancel Order'.\",\n",
    "    'change_order': \"To change your order, go to your order details and select 'Edit Order'.\",\n",
    "    'place_order': \"To place an order, add items to your cart and proceed to checkout.\",\n",
    "    'track_order': \"To track your order, enter your order number in the tracking section on our website.\",\n",
    "    'check_payment_methods': \"To view available payment methods, go to the payment options section during checkout.\",\n",
    "    'payment_issue': \"If you have a payment issue, check your payment details and try again.\",\n",
    "    'check_refund_policy': \"To view our refund policy, visit the 'Refund Policy' page on our website.\",\n",
    "    'get_refund': \"To request a refund, go to your order details and select 'Request Refund'.\",\n",
    "    'track_refund': \"To track your refund, go to the 'Refunds' section in your account.\",\n",
    "    'change_shipping_address': \"To update your shipping address, go to your account settings and select 'Shipping Address'.\",\n",
    "    'set_up_shipping_address': \"To set up a new shipping address, go to your account settings and add a new address in the 'Shipping Address' section.\"\n",
    "}\n",
    "\n",
    "# Replace the intent values with the descriptive sentences\n",
    "df['intent'] = df['intent'].replace(intent_mapping)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'utterance' column\n",
    "df['utterance'] = df['utterance'].apply(preprocess_text)\n",
    "\n",
    "# Create a TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['utterance'])\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Split the data into training and testing sets for TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(df_tfidf, df['intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train_tfidf)\n",
    "y_test_encoded = encoder.transform(y_test_tfidf)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVC': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    # 'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train_encoded)\n",
    "    accuracy = model.score(X_test_tfidf, y_test_encoded)\n",
    "    print(f'TF-IDF {model_name} Model Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Deep Learning Models\n",
    "# Assuming `len(intent_mapping)` gives the number of classes for DL models\n",
    "\n",
    "def build_dl_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(intent_mapping), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dl_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_shape, output_dim=128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(len(intent_mapping), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate deep learning models\n",
    "dl_models = {\n",
    "    'Dense Neural Network': build_dl_model(X_train_tfidf.shape[1]),\n",
    "    'LSTM': build_dl_lstm_model(X_train_tfidf.shape[1])\n",
    "}\n",
    "\n",
    "for model_name, model in dl_models.items():\n",
    "    model.fit(X_train_tfidf, y_train_encoded, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    _, accuracy = model.evaluate(X_test_tfidf, y_test_encoded, verbose=0)\n",
    "    print(f'TF-IDF Deep Learning {model_name} Model Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaddress</th>\n",
       "      <th>aan</th>\n",
       "      <th>aand</th>\n",
       "      <th>abck</th>\n",
       "      <th>abill</th>\n",
       "      <th>aboutgetting</th>\n",
       "      <th>aboutr</th>\n",
       "      <th>aboutt</th>\n",
       "      <th>aboutthe</th>\n",
       "      <th>...</th>\n",
       "      <th>yourpayment</th>\n",
       "      <th>youshow</th>\n",
       "      <th>youtell</th>\n",
       "      <th>youy</th>\n",
       "      <th>yoy</th>\n",
       "      <th>yu</th>\n",
       "      <th>yuou</th>\n",
       "      <th>yyou</th>\n",
       "      <th>zccount</th>\n",
       "      <th>zccounts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1902 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaddress  aan  aand  abck  abill  aboutgetting  aboutr  aboutt  \\\n",
       "0  0.0       0.0  0.0   0.0   0.0    0.0           0.0     0.0     0.0   \n",
       "1  0.0       0.0  0.0   0.0   0.0    0.0           0.0     0.0     0.0   \n",
       "2  0.0       0.0  0.0   0.0   0.0    0.0           0.0     0.0     0.0   \n",
       "3  0.0       0.0  0.0   0.0   0.0    0.0           0.0     0.0     0.0   \n",
       "4  0.0       0.0  0.0   0.0   0.0    0.0           0.0     0.0     0.0   \n",
       "\n",
       "   aboutthe  ...  yourpayment  youshow  youtell  youy  yoy   yu  yuou  yyou  \\\n",
       "0       0.0  ...          0.0      0.0      0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "1       0.0  ...          0.0      0.0      0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "2       0.0  ...          0.0      0.0      0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "3       0.0  ...          0.0      0.0      0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "4       0.0  ...          0.0      0.0      0.0   0.0  0.0  0.0   0.0   0.0   \n",
       "\n",
       "   zccount  zccounts  \n",
       "0      0.0       0.0  \n",
       "1      0.0       0.0  \n",
       "2      0.0       0.0  \n",
       "3      0.0       0.0  \n",
       "4      0.0       0.0  \n",
       "\n",
       "[5 rows x 1902 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Logistic Regression Model Accuracy: 0.9854\n",
      "BoW Decision Tree Model Accuracy: 0.9770\n",
      "BoW Naive Bayes Model Accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "# Assuming necessary NLTK data files are already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = '20000-Utterances-Training-dataset-for-chatbots-virtual-assistant-Bitext-sample.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['category', 'flags']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Define the mapping dictionary\n",
    "intent_mapping = {\n",
    "    'create_account': \"To create a new account, go to the sign-up page and fill in your details.\",\n",
    "    'delete_account': \"To delete your account, navigate to your account settings and select 'Delete Account'.\",\n",
    "    'edit_account': \"To update your account details, go to your profile and select 'Edit Profile'.\",\n",
    "    'recover_password': \"To recover your password, click on 'Forgot Password' and follow the instructions.\",\n",
    "    'registration_problems': \"If you have registration issues, check the provided information and ensure all required fields are filled.\",\n",
    "    'switch_account': \"To switch accounts, log out of your current account and log in with the other account credentials.\",\n",
    "    'check_cancellation_fee': \"To check the cancellation fee, visit the cancellation policy section in our terms and conditions.\",\n",
    "    'contact_customer_service': \"To contact customer service, use the 'Contact Us' form on our website.\",\n",
    "    'contact_human_agent': \"To chat with a human agent, use the live chat feature available on our support page.\",\n",
    "    'delivery_options': \"To view delivery options, go to the shipping section during checkout.\",\n",
    "    'delivery_period': \"To check the delivery period, view the estimated delivery date provided at checkout.\",\n",
    "    'complaint': \"To file a complaint, fill out the complaint form available on our support page.\",\n",
    "    'review': \"To leave a review, go to the product page and click on 'Write a Review'.\",\n",
    "    'check_invoices': \"To check your invoices, log in to your account and go to the 'Invoices' section.\",\n",
    "    'get_invoice': \"To get a copy of your invoice, access the 'Orders' section in your account and select 'View Invoice'.\",\n",
    "    'newsletter_subscription': \"To subscribe to our newsletter, enter your email in the subscription box at the bottom of the homepage.\",\n",
    "    'cancel_order': \"To cancel your order, go to your order history and select 'Cancel Order'.\",\n",
    "    'change_order': \"To change your order, go to your order details and select 'Edit Order'.\",\n",
    "    'place_order': \"To place an order, add items to your cart and proceed to checkout.\",\n",
    "    'track_order': \"To track your order, enter your order number in the tracking section on our website.\",\n",
    "    'check_payment_methods': \"To view available payment methods, go to the payment options section during checkout.\",\n",
    "    'payment_issue': \"If you have a payment issue, check your payment details and try again.\",\n",
    "    'check_refund_policy': \"To view our refund policy, visit the 'Refund Policy' page on our website.\",\n",
    "    'get_refund': \"To request a refund, go to your order details and select 'Request Refund'.\",\n",
    "    'track_refund': \"To track your refund, go to the 'Refunds' section in your account.\",\n",
    "    'change_shipping_address': \"To update your shipping address, go to your account settings and select 'Shipping Address'.\",\n",
    "    'set_up_shipping_address': \"To set up a new shipping address, go to your account settings and add a new address in the 'Shipping Address' section.\"\n",
    "}\n",
    "\n",
    "# Replace the intent values with the descriptive sentences\n",
    "df['intent'] = df['intent'].replace(intent_mapping)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'utterance' column\n",
    "df['utterance'] = df['utterance'].apply(preprocess_text)\n",
    "\n",
    "# Create a BoW representation\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_matrix = bow_vectorizer.fit_transform(df['utterance'])\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Split the data into training and testing sets for BoW\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(df_bow, df['intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVC': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    # 'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_bow, y_train_bow)\n",
    "    accuracy = model.score(X_test_bow, y_test_bow)\n",
    "    print(f'BoW {model_name} Model Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Deep Learning Models\n",
    "def build_dl_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(intent_mapping), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example of a deeper architecture using embeddings and LSTM\n",
    "def build_dl_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_shape, output_dim=128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(len(intent_mapping), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate deep learning models\n",
    "dl_models = {\n",
    "    'Dense Neural Network': build_dl_model(X_train_bow.shape[1]),\n",
    "    'LSTM': build_dl_lstm_model(X_train_bow.shape[1])\n",
    "}\n",
    "\n",
    "for model_name, model in dl_models.items():\n",
    "    model.fit(X_train_bow, y_train_bow, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    _, accuracy = model.evaluate(X_test_bow, y_test_bow, verbose=0)\n",
    "    print(f'BoW Deep Learning {model_name} Model Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Random Forest Model Accuracy: 0.9870\n",
      "Model saved as tfidf_random_forest_model.pkl\n",
      "TF-IDF Vectorizer saved as tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib  # For saving the model as .pkl\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = '20000-Utterances-Training-dataset-for-chatbots-virtual-assistant-Bitext-sample.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['category', 'flags']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Define the mapping dictionary\n",
    "intent_mapping = {\n",
    "    'create_account': \"To create a new account, go to the sign-up page and fill in your details.\",\n",
    "    'delete_account': \"To delete your account, navigate to your account settings and select 'Delete Account'.\",\n",
    "    'edit_account': \"To update your account details, go to your profile and select 'Edit Profile'.\",\n",
    "    'recover_password': \"To recover your password, click on 'Forgot Password' and follow the instructions.\",\n",
    "    'registration_problems': \"If you have registration issues, check the provided information and ensure all required fields are filled.\",\n",
    "    'switch_account': \"To switch accounts, log out of your current account and log in with the other account credentials.\",\n",
    "    'check_cancellation_fee': \"To check the cancellation fee, visit the cancellation policy section in our terms and conditions.\",\n",
    "    'contact_customer_service': \"To contact customer service, use the 'Contact Us' form on our website.\",\n",
    "    'contact_human_agent': \"To chat with a human agent, use the live chat feature available on our support page.\",\n",
    "    'delivery_options': \"To view delivery options, go to the shipping section during checkout.\",\n",
    "    'delivery_period': \"To check the delivery period, view the estimated delivery date provided at checkout.\",\n",
    "    'complaint': \"To file a complaint, fill out the complaint form available on our support page.\",\n",
    "    'review': \"To leave a review, go to the product page and click on 'Write a Review'.\",\n",
    "    'check_invoices': \"To check your invoices, log in to your account and go to the 'Invoices' section.\",\n",
    "    'get_invoice': \"To get a copy of your invoice, access the 'Orders' section in your account and select 'View Invoice'.\",\n",
    "    'newsletter_subscription': \"To subscribe to our newsletter, enter your email in the subscription box at the bottom of the homepage.\",\n",
    "    'cancel_order': \"To cancel your order, go to your order history and select 'Cancel Order'.\",\n",
    "    'change_order': \"To change your order, go to your order details and select 'Edit Order'.\",\n",
    "    'place_order': \"To place an order, add items to your cart and proceed to checkout.\",\n",
    "    'track_order': \"To track your order, enter your order number in the tracking section on our website.\",\n",
    "    'check_payment_methods': \"To view available payment methods, go to the payment options section during checkout.\",\n",
    "    'payment_issue': \"If you have a payment issue, check your payment details and try again.\",\n",
    "    'check_refund_policy': \"To view our refund policy, visit the 'Refund Policy' page on our website.\",\n",
    "    'get_refund': \"To request a refund, go to your order details and select 'Request Refund'.\",\n",
    "    'track_refund': \"To track your refund, go to the 'Refunds' section in your account.\",\n",
    "    'change_shipping_address': \"To update your shipping address, go to your account settings and select 'Shipping Address'.\",\n",
    "    'set_up_shipping_address': \"To set up a new shipping address, go to your account settings and add a new address in the 'Shipping Address' section.\"\n",
    "}\n",
    "\n",
    "# Replace the intent values with the descriptive sentences\n",
    "df['intent'] = df['intent'].replace(intent_mapping)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'utterance' column\n",
    "df['utterance'] = df['utterance'].apply(preprocess_text)\n",
    "\n",
    "# Create a TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['utterance'])\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Split the data into training and testing sets for TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(df_tfidf, df['intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = rf_classifier.score(X_test_tfidf, y_test_tfidf)\n",
    "print(f'TF-IDF Random Forest Model Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the trained model as a .pkl file\n",
    "joblib.dump(rf_classifier, 'tfidf_random_forest_model.pkl')\n",
    "print(\"Model saved as tfidf_random_forest_model.pkl\")\n",
    "\n",
    "# Save the TF-IDF vectorizer as a .pkl file\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "print(\"TF-IDF Vectorizer saved as tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent: To delete your account, navigate to your account settings and select 'Delete Account'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaide\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib  # For loading the model and vectorizer\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load TF-IDF Vectorizer and Random Forest model\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "rf_classifier = joblib.load('tfidf_random_forest_model.pkl')\n",
    "\n",
    "# Example input\n",
    "input_text = \"How do I delete my account?\"\n",
    "\n",
    "# Preprocess the input text\n",
    "preprocessed_input = preprocess_text(input_text)\n",
    "\n",
    "# Transform the preprocessed text using the loaded TF-IDF vectorizer\n",
    "input_vector = tfidf_vectorizer.transform([preprocessed_input])\n",
    "\n",
    "# Predict the intent using the loaded Random Forest model\n",
    "predicted_intent = rf_classifier.predict(input_vector)[0]\n",
    "\n",
    "# Output the predicted intent\n",
    "print(f\"Predicted Intent: {predicted_intent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
